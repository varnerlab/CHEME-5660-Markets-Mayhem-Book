
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Markov decision process (MDPs) &#8212; CHEME 5660</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://varnerlab.github.io/CHEME-5660-Markets-Mayhem-Book/infrastructure.html/chapter-4-dir/mdp.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="../References.html" />
    <link rel="prev" title="Unit 4: Decisions" href="chapter-4.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/cornell_seal_simple_black.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CHEME 5660</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../infrastructure.html">
                    CHEME 5660 Course Outline and Information
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../julia-dir/julia-landing.html">
   Getting Started with Julia
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../julia-dir/julia-installation.html">
     Julia Installation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../julia-dir/julia-basics.html">
     Introduction to Julia Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../julia-dir/julia-data.html">
     Working with Data in Julia
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-1-dir/chapter-1.html">
   Unit 1. The Financial System
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-1-dir/global.html">
     Personal Finance Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-1-dir/assets.html">
     Financial Balances and Abstract Assets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-1-dir/markets.html">
     Markets, Exchanges and Investment Instruments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-2-dir/chapter-2.html">
   Unit 2. Modeling and Simulation of Asset Price Dynamics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-2-dir/probability-random-variables.html">
     Probability, Random Variables and Stochastic Processes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-2-dir/stochastic-differential-equations.html">
     Macroscopic Market Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-2-dir/agent-based-market-models.html">
     Microscopic Market Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../chapter-3-dir/chapter-3-landing.html">
   Unit 3. Risk, Portfolios, Derivatives, and Leverage
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-3-dir/bonds.html">
     Fixed Income Debt Securities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-3-dir/contracts.html">
     Derivative Securities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../chapter-3-dir/markowitz.html">
     Equity, Debt and Modern Portfolio Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="chapter-4.html">
   Unit 4. Decisions
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Markov decision process (MDPs)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/varnerlab/CHEME-5660-Markets-Mayhem-Book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/varnerlab/CHEME-5660-Markets-Mayhem-Book/issues/new?title=Issue%20on%20page%20%2Fchapter-4-dir/mdp.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/chapter-4-dir/mdp.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Markov decision process (MDPs)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chains">
     Markov chains
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discrete-time-markov-chains">
       Discrete time Markov chains
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hidden-markov-models-hmms">
       Hidden Markov Models (HMMs)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-decision-processes-mdps">
     Markov decision processes (MDPs)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#policy-evaluation">
       Policy evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-function-policies">
       Value function policies
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-iteration">
       Value iteration
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Markov decision process (MDPs)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Markov decision process (MDPs)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chains">
     Markov chains
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discrete-time-markov-chains">
       Discrete time Markov chains
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hidden-markov-models-hmms">
       Hidden Markov Models (HMMs)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-decision-processes-mdps">
     Markov decision processes (MDPs)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#policy-evaluation">
       Policy evaluation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-function-policies">
       Value function policies
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#value-iteration">
       Value iteration
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="markov-decision-process-mdps">
<h1>Markov decision process (MDPs)<a class="headerlink" href="#markov-decision-process-mdps" title="Permalink to this headline">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>A Markov decision process (MDP) provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs take their name from the Russian mathematician <a class="reference external" href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a>,  as they are an extension of <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>, a stochastic model which describes a sequence of possible events in which the probability of each event depends only on the system state in the previous event.</p>
<p>In this lecture:</p>
<ul class="simple">
<li><p>We will discuss <a class="reference internal" href="#content-references-markov-chains"><span class="std std-ref">Markov chains</span></a> and discrete time <a class="reference internal" href="#content-references-structure-of-an-hmm"><span class="std std-ref">Hidden Markov Models (HMMs)</span></a>, which are approaches for modeling the evolution of a stochastic system as a series of possible events.</p></li>
<li><p>We will also discuss <a class="reference internal" href="#content-references-structure-of-an-mdp"><span class="std std-ref">Markov decision processes (MDPs)</span></a>, which is an approach for making decisions in a probabilistic world.</p></li>
</ul>
<hr class="docutils" />
</section>
<section id="markov-chains">
<span id="content-references-markov-chains"></span><h2>Markov chains<a class="headerlink" href="#markov-chains" title="Permalink to this headline">#</a></h2>
<p>A Markov chain is a stochastic model describing a sequence of possible events where the probability of each of these events depends only on the current state of the system, and not on past system states. A system’s state space and time (much like probability) can be either discrete or continuous; for most of the applications we’ll be interested in, we’ll focus on discrete time and discrete finite state spaces.</p>
<section id="discrete-time-markov-chains">
<span id="content-references-discrete-time-markov-chains"></span><h3>Discrete time Markov chains<a class="headerlink" href="#discrete-time-markov-chains" title="Permalink to this headline">#</a></h3>
<p>A discrete-time Markov chain is a sequence of random variables <span class="math notranslate nohighlight">\(X_{1}\)</span>, <span class="math notranslate nohighlight">\(X_{2}\)</span>, <span class="math notranslate nohighlight">\(X_{3}\)</span>, …, <span class="math notranslate nohighlight">\(X_{n}\)</span> that have the <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a>, i.e., the probability of moving to the <em>next state</em> depends only on the <em>present state</em> and not on the <em>previous states</em>:</p>
<div class="math notranslate nohighlight" id="equation-eqn-markov-property">
<span class="eqno">(72)<a class="headerlink" href="#equation-eqn-markov-property" title="Permalink to this equation">#</a></span>\[P(X_{n+1} = x | X_{1}=x_{1}, \dots, X_{n}=x_{n}) = P(X_{n+1} = x | X_{n}=y)\]</div>
<p>where <em>states</em> refer to a finite set of discrete values that the system can exist in.  If the state space is finite, the transition probability distribution, i.e., the probability of moving from state(s) <span class="math notranslate nohighlight">\(i\rightarrow{j}\)</span>, can be encoded in the transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>. Elements of <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>, denoted as <span class="math notranslate nohighlight">\(p_{ij}\)</span>, encode the probability of moving from state <span class="math notranslate nohighlight">\(i\rightarrow{j}\)</span> during the next time step:</p>
<div class="math notranslate nohighlight" id="equation-eqn-transition-prob-matrix">
<span class="eqno">(73)<a class="headerlink" href="#equation-eqn-transition-prob-matrix" title="Permalink to this equation">#</a></span>\[p_{ij} = P(X_{n+1}~=~j~|~X_{n}~=~i)\]</div>
<p>The transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> has a few interesting properties. First, the rows of transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> must sum to unity, i.e., each row encodes the probability of all possible outcomes, thus it must sum to one. Second, if the transition matrix  <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is time invariant, then <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is the same at each step. This leads to <a class="reference internal" href="#obs-n-transition">Observation 11</a>:</p>
<div class="proof observation admonition" id="obs-n-transition">
<p class="admonition-title"><span class="caption-number">Observation 11 </span> (Time-invariant state transition)</p>
<section class="observation-content" id="proof-content">
<p>A Markov chain have finite state set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> and the time-invariant state transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>. Further, let the state vector at time <span class="math notranslate nohighlight">\(j\)</span> be given by <span class="math notranslate nohighlight">\(\mathbf{x}_{j}\)</span>, where <span class="math notranslate nohighlight">\(x_{s,j}\geq{0},\forall{s}\in\mathcal{S}\)</span> and:</p>
<div class="math notranslate nohighlight">
\[\sum_{s\in\mathcal{S}}x_{s,j} = 1\qquad\forall{j}\]</div>
<p>Then, the state of the Markov chain at time step <span class="math notranslate nohighlight">\(n+1\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}_{n+1} = \mathbf{x}_{n}\mathbf{P}^n\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span> denotes the system state vector at time step <span class="math notranslate nohighlight">\(n\)</span>.</p>
</section>
</div><p>Finally, if the Markov chain is both time-invarient and non-periodic, there exists a unique stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{P}^{k}\)</span> converges to a rank-one matrix in which each row is the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[\lim_{k\rightarrow\infty} \mathbf{P}^{k} = \mathbf{1}\pi\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is a column vector of all 1’s. Let’s consider an example to make these ideas less abstract.</p>
<div class="proof example admonition" id="example-dicrete-mchain">
<p class="admonition-title"><span class="caption-number">Example 30 </span> (Discrete Markov chain simulation)</p>
<section class="example-content" id="proof-content">
<figure class="align-default" id="fig-discrete-markov-model">
<a class="reference internal image-reference" href="../_images/Fig-Ex-Discrete-Markov-Model.pdf"><img alt="../_images/Fig-Ex-Discrete-Markov-Model.pdf" src="../_images/Fig-Ex-Discrete-Markov-Model.pdf" style="height: 120px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 41 </span><span class="caption-text">Schematic of a discrete two-state time-invariant Markov model; <span class="math notranslate nohighlight">\(p_{ij}\)</span> denotes the time-invariant transition probability between state <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.</span><a class="headerlink" href="#fig-discrete-markov-model" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Consider the time-invariant two-state Discrete Markov chain with state transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{P} = \begin{bmatrix}
0.9 &amp; 0.1 \\
0.6 &amp; 0.4 \\
\end{bmatrix}
\end{split}\]</div>
<p>shown in (<a class="reference internal" href="#fig-discrete-markov-model"><span class="std std-numref">Fig. 41</span></a>). The transition matrix admits a stationary (non-periodic) solution. As the number of iterations <span class="math notranslate nohighlight">\(n\)</span> becomes large the system state converges to a stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span>; for <span class="math notranslate nohighlight">\(n&gt;13\)</span> the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\pi = (0.8571, 0.1428)\]</div>
<p>Thus, regardless of the starting state of this Markov chain, the long-term behavior is given by the stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>source</strong>: Fill me in.</p>
</section>
</div></section>
<section id="hidden-markov-models-hmms">
<span id="content-references-structure-of-an-hmm"></span><h3>Hidden Markov Models (HMMs)<a class="headerlink" href="#hidden-markov-models-hmms" title="Permalink to this headline">#</a></h3>
<p>Hidden Markov models (HMMs) are statistical models in which the system being modeled is assumed to be a Markov process with unobservable states but observable outcomes. HMMs have the same structural components as a standard Markov chain model, but each hidden state can be thought of as sending an observable single. HMMs are widely used in many disciplines to model uncertain systems and situations.</p>
<p>Let’s build upon <a class="reference internal" href="#example-dicrete-mchain">Example 30</a> and construct an HMM that mimics a <a class="reference internal" href="../chapter-3-dir/contracts.html"><span class="doc std std-doc">CRR binomial lattice</span></a>:</p>
<div class="proof example admonition" id="example-dicrete-mchain-hmm">
<p class="admonition-title"><span class="caption-number">Example 31 </span> (Stationary hidden Markov model)</p>
<section class="example-content" id="proof-content">
<figure class="align-default" id="fig-discrete-hidden-markov-model">
<a class="reference internal image-reference" href="../_images/Fig-Ex-Discrete-Hidden-Markov-Model.pdf"><img alt="../_images/Fig-Ex-Discrete-Hidden-Markov-Model.pdf" src="../_images/Fig-Ex-Discrete-Hidden-Markov-Model.pdf" style="height: 280px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 42 </span><span class="caption-text">Schematic of a discrete two-state time-invariant hidden Markov model (HMM); <span class="math notranslate nohighlight">\(p_{ij}\)</span> denotes the time-invariant transition probability between state <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.</span><a class="headerlink" href="#fig-discrete-hidden-markov-model" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Consider the time-invariant two-state Discrete Markov chain with state transition matrix <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{P} = \begin{bmatrix}
0.7 &amp; 0.3 \\
0.6 &amp; 0.4 \\
\end{bmatrix}
\end{split}\]</div>
<p>shown in (<a class="reference internal" href="#fig-discrete-hidden-markov-model"><span class="std std-numref">Fig. 42</span></a>). Let <span class="math notranslate nohighlight">\(Y_{1}\)</span> denote the observable value for state 1, while <span class="math notranslate nohighlight">\(Y_{2}\)</span> denotes the observable value of
state 2. The transition matrix admits the stationary (non-periodic) stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> given by:</p>
<div class="math notranslate nohighlight">
\[\pi = (0.66, 0.33)\]</div>
<p>Suppose the unobservable state <span class="math notranslate nohighlight">\(X_{1}\)</span> was the <code class="docutils literal notranslate"><span class="pre">up</span></code> state; when the system is in state 1 we observe an up move of size:</p>
<div class="math notranslate nohighlight">
\[u = \exp(\sigma\sqrt{\Delta{t}})\]</div>
<p>On the other hand, let state 2 denote the <code class="docutils literal notranslate"><span class="pre">down</span></code> state; when the system is in state 2 we observe a down move of size:</p>
<div class="math notranslate nohighlight">
\[d = \exp(-\sigma\sqrt{\Delta{t}})\]</div>
<p><strong>Simulation</strong></p>
<figure class="align-default" id="fig-discrete-hidden-markov-model-sim-up">
<a class="reference internal image-reference" href="../_images/Fig-HMM-up-SRA-N250.pdf"><img alt="../_images/Fig-HMM-up-SRA-N250.pdf" src="../_images/Fig-HMM-up-SRA-N250.pdf" style="height: 360px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 43 </span><span class="caption-text">Simulation of an asset price governed by <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>. Solid line denotes the mean predicted price, shaded regions denote the 68% and 95% confidence regions. <em>parameters</em>: <span class="math notranslate nohighlight">\(S_{o}\)</span> = 100, <span class="math notranslate nohighlight">\(\Delta{T}\)</span> = (1/365), <span class="math notranslate nohighlight">\(T\)</span> = 45 days and IV = 24.20%.
The trajectory was computed by sampling a <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> distribution constructed from the HMM stationary distribution.</span><a class="headerlink" href="#fig-discrete-hidden-markov-model-sim-up" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>source</strong>: <a class="reference external" href="https://github.com/varnerlab/CHEME-5660-Markets-Mayhem-Example-Notebooks">download the live Jupyter notebook from GitHub</a></p>
</section>
</div></section>
</section>
<section id="markov-decision-processes-mdps">
<span id="content-references-structure-of-an-mdp"></span><h2>Markov decision processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Permalink to this headline">#</a></h2>
<p>A Markov decision process (MDP) is an approach for modeling decision-making in situations where outcomes are partly random but also partly under the control of a <em>decision-maker</em> who receives a reward (or penalty) for each decision. Thus, unlike <a class="reference internal" href="#content-references-discrete-time-markov-chains"><span class="std std-ref">Discrete time Markov chains</span></a>, Markov decision processes (MDPs) involve actions, which allow choice, and rewards that provide motivation for the decision maker.</p>
<p>At each time step, let’s assume the system in some state <span class="math notranslate nohighlight">\(s\)</span> in a set of possible states <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span> and the decision maker may choose any action <span class="math notranslate nohighlight">\(a\)</span> from a set of possible actions <span class="math notranslate nohighlight">\(a\in\mathcal{A}\)</span> that are available in state <span class="math notranslate nohighlight">\(s\)</span>. The system responds at the next time step by <em>potentially</em> moving into a new state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> and rewarding the decision maker <span class="math notranslate nohighlight">\(R_{a}\left(s, s^{\prime}\right)\)</span>. The probability that the system moves into a new state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> depends upon the chosen action <span class="math notranslate nohighlight">\(a\)</span> and the current state <span class="math notranslate nohighlight">\(s\)</span>; this probability is governed by a state transition function <span class="math notranslate nohighlight">\(P_{a}\left(s,s^{\prime}\right)\)</span>.</p>
<div class="proof definition admonition" id="defn-formal-mdp">
<p class="admonition-title"><span class="caption-number">Definition 46 </span> (Markov decision tuple )</p>
<section class="definition-content" id="proof-content">
<p>A Markov decision process is the tuple <span class="math notranslate nohighlight">\(\left(\mathcal{S}, \mathcal{A}, R_{a}\left(s, s^{\prime}\right), T_{a}\left(s,s^{\prime}\right), \gamma\right)\)</span> where:</p>
<ul class="simple">
<li><p>The state space <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is the set of all possible states <span class="math notranslate nohighlight">\(s\)</span> that a system can exist in</p></li>
<li><p>The action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is the set of all possible actions <span class="math notranslate nohighlight">\(a\)</span> that are available to the agent, where <span class="math notranslate nohighlight">\(\mathcal{A}_{s} \subseteq \mathcal{A}\)</span> is the subset of the action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> that is accessible from state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>An expected immediate reward <span class="math notranslate nohighlight">\(R_{a}\left(s, s^{\prime}\right)\)</span> is received after transitioning from state <span class="math notranslate nohighlight">\(s\rightarrow{s}^{\prime}\)</span> due to action <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>The transition <span class="math notranslate nohighlight">\(T_{a}\left(s,s^{\prime}\right) = P(s_{t+1} = s^{\prime}~|~s_{t}=s,a_{t} = a)\)</span> denotes the probability that action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> will result in state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> at time <span class="math notranslate nohighlight">\(t+1\)</span></p></li>
<li><p>The quantity <span class="math notranslate nohighlight">\(\gamma\)</span> is a <em>discount factor</em>; the discount factor is used to weight the <em>future expected utility</em>.</p></li>
</ul>
<p>Finally, a policy function <span class="math notranslate nohighlight">\(\pi\)</span> is the (potentially probabilistic) mapping from states <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span> to actions <span class="math notranslate nohighlight">\(a\in\mathcal{A}\)</span> used by the agent to solve the decision task.</p>
</section>
</div><section id="policy-evaluation">
<h3>Policy evaluation<a class="headerlink" href="#policy-evaluation" title="Permalink to this headline">#</a></h3>
<p>One immediate question that jumps out is what is a policy function <span class="math notranslate nohighlight">\(\pi\)</span>, and how do we find the best possible policy for our decision problem? To do this, we need a way to estimate how good (or bad) a particular policy is; the approach we use is called <em>policy evaluation</em>. Let’s denote the expected utility gained by executing some policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> from state <span class="math notranslate nohighlight">\(s\)</span> as <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span>. Then, an <em>optimal policy</em> function <span class="math notranslate nohighlight">\(\pi^{\star}\)</span> is one that maximizes the expected utility:</p>
<div class="math notranslate nohighlight">
\[\pi^{\star}\left(s\right) = \text{arg max}~U^{\pi}(s)\]</div>
<p>for all <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span>. We can iteratively compute the utility of a policy <span class="math notranslate nohighlight">\(\pi\)</span>. If the agent makes a single move, the utility will be the reward the agent receives by implementing policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[U_{1}^{\pi}(s) = R(s,\pi(s))\]</div>
<p>However, if we let the agent perform two, three, or <span class="math notranslate nohighlight">\(k\)</span> possible iterations, we get a <em>lookahead</em> equation which relates the value of
the utility at iteration <span class="math notranslate nohighlight">\(k\)</span> to <span class="math notranslate nohighlight">\(k+1\)</span>:</p>
<div class="math notranslate nohighlight">
\[U_{k+1}^{\pi}(s) = R(s,\pi(s)) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, \pi(s))U_{k}^{\pi}(s^{\prime})\]</div>
<p>As <span class="math notranslate nohighlight">\(k\rightarrow\infty\)</span> the lookahead utility converges to a stationary value <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span>:</p>
<div class="proof definition admonition" id="defn-policy-evalution">
<p class="admonition-title"><span class="caption-number">Definition 47 </span> (Value function)</p>
<section class="definition-content" id="proof-content">
<p>Suppose we have a Markov decision process with the tuple <span class="math notranslate nohighlight">\(\left(\mathcal{S}, \mathcal{A}, R_{a}\left(s, s^{\prime}\right), T_{a}\left(s,s^{\prime}\right), \gamma\right)\)</span>. Then, the utility of then policy function <span class="math notranslate nohighlight">\(\pi\)</span> equals:</p>
<div class="math notranslate nohighlight" id="equation-eqn-converged-policy-eval">
<span class="eqno">(74)<a class="headerlink" href="#equation-eqn-converged-policy-eval" title="Permalink to this equation">#</a></span>\[U^{\pi}(s) = R(s,\pi(s)) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, \pi(s))U^{\pi}(s^{\prime})\]</div>
</section>
</div><p>Let’s do an example to illustrate policy evaluation:</p>
<div class="proof example admonition" id="example-MDP-line">
<p class="admonition-title"><span class="caption-number">Example 32 </span> (Tiger problem)</p>
<section class="example-content" id="proof-content">
<figure class="align-default" id="fig-linear-mdp-schematic">
<a class="reference internal image-reference" href="../_images/Fig-Linear-MDP-Schematic.pdf"><img alt="../_images/Fig-Linear-MDP-Schematic.pdf" src="../_images/Fig-Linear-MDP-Schematic.pdf" style="height: 110px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 44 </span><span class="caption-text">Schematic of the Tiger problem modeled as an N-state, two-action Markov decision process. A tiger hides behind the red door while freedom awaits behind the green door.</span><a class="headerlink" href="#fig-linear-mdp-schematic" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>An agent trapped in a long hallway with two doors at either end (<a class="reference internal" href="#fig-linear-mdp-schematic"><span class="std std-numref">Fig. 44</span></a>). Behind the red door is a tiger (and certain death), while behind the green door is freedom. If the agent opens the red door, the agent is eaten (and receives a large negative reward). However, if the agent opens the green door, it escapes and gets a positive reward.</p>
<p>For this problem, the MDP has the tuple components:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S} = \left\{1,2,\dots,N\right\}\)</span> while the action set is <span class="math notranslate nohighlight">\(\mathcal{A} = \left\{a_{1},a_{2}\right\}\)</span>; action <span class="math notranslate nohighlight">\(a_{1}\)</span> moves the agent one state to the right, action <span class="math notranslate nohighlight">\(a_{2}\)</span> moves the agent one state to the left.</p></li>
<li><p>The agent receives a reward of +10 for entering state 1 (escapes). However, the agent is penalized -100 for entering state N (eaten by the tiger).  Finally, the agent is not charged to move to adjacent locations.</p></li>
<li><p>Let the probability of correctly executing the action <span class="math notranslate nohighlight">\(a_{j}\)</span> be <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
</ul>
<p>Let’s compute <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span> for different choices for the policy function <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>source</strong>: <a class="reference external" href="https://github.com/varnerlab/CHEME-5660-Markets-Mayhem-Example-Notebooks">download the live Jupyter notebook from GitHub</a></p>
</section>
</div><p>The utility associated with an optimal policy <span class="math notranslate nohighlight">\(\pi^{\star}\)</span> is called the optimal utility <span class="math notranslate nohighlight">\(U^{\star}\)</span>.</p>
</section>
<section id="value-function-policies">
<h3>Value function policies<a class="headerlink" href="#value-function-policies" title="Permalink to this headline">#</a></h3>
<p><a class="reference internal" href="#defn-policy-evalution">Definition 47</a> gives us a method to compute the utility for a particular policy <span class="math notranslate nohighlight">\(U^{\pi}(s)\)</span>.
However, suppose we were given the utility, and wanted to estimate the policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> from that utility.
Given a utility <span class="math notranslate nohighlight">\(U\)</span>, we can estimate a policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> using the <span class="math notranslate nohighlight">\(Q\)</span>-function (action-value function):</p>
<div class="math notranslate nohighlight" id="equation-eqn-action-value-function">
<span class="eqno">(75)<a class="headerlink" href="#equation-eqn-action-value-function" title="Permalink to this equation">#</a></span>\[Q(s,a) = R(s,a) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, a)U(s^{\prime})\]</div>
<p>Equation <a class="reference internal" href="#equation-eqn-action-value-function">(75)</a> gives a <span class="math notranslate nohighlight">\(|\mathcal{S}|\times|\mathcal{A}|\)</span> array, where the utility is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-utility-from-q">
<span class="eqno">(76)<a class="headerlink" href="#equation-eqn-utility-from-q" title="Permalink to this equation">#</a></span>\[U(s) = \max_{a} Q(s,a)\]</div>
<p>and the policy <span class="math notranslate nohighlight">\(\pi(s)\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-eqn-policy-from-q">
<span class="eqno">(77)<a class="headerlink" href="#equation-eqn-policy-from-q" title="Permalink to this equation">#</a></span>\[\pi(s) = \text{arg}\max_{a}Q(s,a)\]</div>
</section>
<section id="value-iteration">
<h3>Value iteration<a class="headerlink" href="#value-iteration" title="Permalink to this headline">#</a></h3>
<p>In the previous section, we saw how we could develop <em>a policy</em> <span class="math notranslate nohighlight">\(\pi(s)\)</span> by looking at the values in the <span class="math notranslate nohighlight">\(Q\)</span>-array. However, this required the utility vector; thus, we needed to hypothesize a policy that may not be the <em>optimal policy</em>. There are two techniques to compute optimal policies, and we’ll explore the simpler of the two, namely <em>value iteration</em>.</p>
<p>In <em>value iteration</em>, the value function (the vector of utility values) is updated iteratively using the <em>Bellman update</em>_ procedure:</p>
<div class="math notranslate nohighlight">
\[U_{k+1}(s) = \max_{a}\left(R(s,a) + \gamma\sum_{s^{\prime}\in\mathcal{S}}T(s^{\prime} | s, a)U_{k}(s^{\prime})\right)\]</div>
<p>This procedure is guaranteed to converge to the optimal utility vector (value function).  Let’s modify our original Tiger problem implementation to explore sub-optimal versus optimal policies.</p>
<div class="proof example admonition" id="example-MDP-line-mod">
<p class="admonition-title"><span class="caption-number">Example 33 </span> (Modified Tiger problem)</p>
<section class="example-content" id="proof-content">
<figure class="align-default" id="fig-branched-mdp-schematic-mod">
<a class="reference internal image-reference" href="../_images/Fig-Branched-MDP-Schematic.pdf"><img alt="../_images/Fig-Branched-MDP-Schematic.pdf" src="../_images/Fig-Branched-MDP-Schematic.pdf" style="height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 45 </span><span class="caption-text">Schematic of the Tiger problem modeled as an N-state, four-action (left, right, up, down) Markov decision process. The hallway has three types of paths: unobstructed (white, free), mildly obstructed (light gray, small cost), and obstructed (dark gray, large cost).</span><a class="headerlink" href="#fig-branched-mdp-schematic-mod" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>An agent is trapped in a long hallway with two doors at either end (<a class="reference internal" href="#fig-branched-mdp-schematic-mod"><span class="std std-numref">Fig. 45</span></a>). Behind the green door is a tiger (and certain death), while behind the red door is freedom. If the agent opens the green door, the agent is eaten (and receives a large negative reward). However, if the agent opens the red door, it escapes and gets a positive reward.</p>
<p>For this problem, the MDP has the tuple components:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{S} = \left\{1,2,\dots,N\right\}\)</span> while the action set is <span class="math notranslate nohighlight">\(\mathcal{A} = \left\{a_{1},a_{2}, a_{3}, a_{4}\right\}\)</span>; action <span class="math notranslate nohighlight">\(a_{1}\)</span> moves the agent one state to the left, action <span class="math notranslate nohighlight">\(a_{2}\)</span> moves the agent one state to the right, action <span class="math notranslate nohighlight">\(a_{3}\)</span> moves the agent one stop up, and action <span class="math notranslate nohighlight">\(a_{4}\)</span> moves the agent one step down.</p></li>
<li><p>The agent receives a postive reward for entering the red state <span class="math notranslate nohighlight">\(N\)</span> (escapes). However, the agent is penalized for entering the green state <span class="math notranslate nohighlight">\(1\)</span> (eaten by the tiger).  Finally, the agent is not charged to move to adjacent locations if those locations are unobstructed. However, there is a small charge to move through mildly obstructed locations (light gray circles) and a larger charge to move through obstructed areas (dark gray circles).</p></li>
<li><p>Let the probability of correctly executing an action <span class="math notranslate nohighlight">\(a_{j}\in\mathcal{A}\)</span> be <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
</ul>
<p>Let’s use value iteration to estimate the <em>optimal policy</em> <span class="math notranslate nohighlight">\(\pi^{\star}(s)\)</span></p>
<p><strong>source:</strong> <a class="reference external" href="https://github.com/varnerlab/CHEME-5660-Markets-Mayhem-Example-Notebooks">download the live Jupyter notebook from GitHub</a></p>
</section>
</div></section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h1>
<p>A Markov decision process (MDP) provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs take their name from the Russian mathematician <a class="reference external" href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a>,  as they are an extension of <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>, a stochastic model which describes a sequence of possible events in which the probability of each event depends only on the system state in the previous event.</p>
<p>In this lecture:</p>
<ul class="simple">
<li><p>We discussed <a class="reference internal" href="#content-references-markov-chains"><span class="std std-ref">Markov chains</span></a> and discrete time <a class="reference internal" href="#content-references-structure-of-an-hmm"><span class="std std-ref">Hidden Markov Models (HMMs)</span></a>, which are approaches for modeling the evolution of a stochastic system as a series of possible events. We developed a hidden Markov model for the nodes in a binomial lattice model.</p></li>
<li><p>We also discussed <a class="reference internal" href="#content-references-structure-of-an-mdp"><span class="std std-ref">Markov decision processes (MDPs)</span></a>, which is an approach for making decisions in a probabilistic world. In particular, we
introduced tools to compute the utility of a decision-making policy and the value iteration approach for estimating optimal decision-making policies.</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapter-4-dir"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="chapter-4.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Unit 4: Decisions</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../References.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Varner and Woltornist<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>